{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Dependencies\n",
    "# %pip install -qU datasets pinecone-client sentence-transformers torch pyngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations\n",
    "### Retriever Model\n",
    "The retriever model converts text snippets into vectors and vice versa. The model used is a pretrained\n",
    "  -  @inproceedings{reimers-2019-sentence-bert,\n",
    "    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n",
    "    author = \"Reimers, Nils and Gurevych, Iryna\",\n",
    "    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n",
    "    month = \"11\",\n",
    "    year = \"2019\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://arxiv.org/abs/1908.10084\",\n",
    "    }\n",
    "### Generator Model\n",
    "This model is a generative ai that generates text upon input. The model used is BART pretrained for Q&A\n",
    "- https://yjernite.github.io/lfqa.html\n",
    "### Tutorial\n",
    "As we are complete beginners, we heavily utilized the youtube video \"Open Source Generative AI in Question-Answering (NLP) using Python\" by James Brigg for the AI part\n",
    "- https://invidious.private.coffee/watch?v=L8U-pm-vZ4c\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import pinecone\n",
    "from config import PINECONE_API_KEY\n",
    "\n",
    "#Connect to vector DB\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=\"gcp-starter\"\n",
    ")\n",
    "#basically a table in a db except it's not sql, hardcoded here because\n",
    "# free plan only lets you make one anyways\n",
    "index = pinecone.Index('genghis-khan')\n",
    "\n",
    "# set device to GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# load the retriever model from huggingface model hub\n",
    "retriever = SentenceTransformer(\n",
    "    \"flax-sentence-embeddings/all_datasets_v3_mpnet-base\", device=device)\n",
    "retriever\n",
    "\n",
    "\n",
    "# load generator model from huggingface\n",
    "tokenizer = BartTokenizer.from_pretrained('vblagoje/bart_lfqa')\n",
    "generator = BartForConditionalGeneration.from_pretrained(\n",
    "    'vblagoje/bart_lfqa').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# Helper functions for retrieval and answering\n",
    "\n",
    "# returns the k vectors that most match the query in vector db\n",
    "def query_pinecone(query, top_k):\n",
    "    # generate embeddings for the query\n",
    "    query = retriever.encode([query]).tolist()\n",
    "    # return relevant context vecros from query\n",
    "    context = index.query(query, top_k=top_k, include_metadata=True)\n",
    "    return context\n",
    "\n",
    "\n",
    "# this function checks if relevant passages for the query exists\n",
    "# pinecone gives a \"score\" metric which shows vector similarity \n",
    "def check_relevancy(query):\n",
    "    query = retriever.encode([query]).tolist()\n",
    "    context = index.query(query, top_k=1, include_metadata=True)\n",
    "\n",
    "    # Access the score value\n",
    "    score = context['matches'][0]['score']\n",
    "\n",
    "    # Check if the score is greater than 0.5\n",
    "    # this is an arbitrary value, but I've seen that\n",
    "    # a high match is around 0.8 while low is around 0.3\n",
    "    return score\n",
    "\n",
    "\n",
    "def generate_answer(query, context):\n",
    "    # extract passage_text from Pinecone search result and add the <P> tag\n",
    "    context = [f\"<P> {m['metadata']['passage_text']}\" for m in context]\n",
    "    # concatinate all context passages\n",
    "    context = \" \".join(context)\n",
    "\n",
    "    # contcatinate the query and context passages\n",
    "    input_text = f\"question: {query} context: {context}\"\n",
    "\n",
    "    # tokenize the query to get input_ids\n",
    "    input_tokens = tokenizer([input_text], max_length=1024,\n",
    "                       return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # use generator to predict output ids\n",
    "    ids = generator.generate(\n",
    "        input_tokens[\"input_ids\"], num_beams=2, min_length=20, max_length=100)\n",
    "        \n",
    "    # use tokenizer to decode the output ids\n",
    "    answer = tokenizer.batch_decode(\n",
    "        ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from flask import Flask, request, jsonify, make_response\n",
    "from werkzeug.serving import make_server\n",
    "from pyngrok import ngrok\n",
    "import threading\n",
    "\n",
    "# Create App\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "def run_app():\n",
    "    global server\n",
    "    server = make_server('127.0.0.1', 5000, app)\n",
    "    server.serve_forever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Routes\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pinecone\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return \"Api works!\"\n",
    "\n",
    "\n",
    "@app.route(\"/test_route\")\n",
    "def test_route():\n",
    "    return jsonify(\"test\")\n",
    "\n",
    "@app.route('/ask', methods=['POST'])\n",
    "def ask():\n",
    "    try:\n",
    "        if request.method == 'POST':\n",
    "            # Get the search query from the form\n",
    "            query = request.form['search_query']\n",
    "\n",
    "            context = query_pinecone(query, top_k=3)\n",
    "            #debug statement to view context and scores\n",
    "            print(context)\n",
    "\n",
    "            relevancy_of_text = check_relevancy(query)\n",
    "            if relevancy_of_text>0.5:\n",
    "                matching_context = context[\"matches\"]\n",
    "                answer = generate_answer(query, matching_context)\n",
    "                citations = [doc[\"metadata\"]['passage_text']\n",
    "                             for doc in matching_context]\n",
    "                return jsonify({\"answer\": answer, \"citations\": citations, \"confidence\":relevancy_of_text})\n",
    "            else:\n",
    "                return jsonify({\"message\": \"No relevant text found\"})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred: \", e)\n",
    "        return jsonify({'error': 'Internal Server Error'}), 500\n",
    "\n",
    "@app.route('/vectorize_page', methods=['POST'])\n",
    "\n",
    "# performance: the genghis khan wikipedia page takes around 40 secs\n",
    "def vectorize_page():\n",
    "    try:\n",
    "        # Get the JSON data from the request body\n",
    "        data = request.json\n",
    "        print(\"data: \" + str(data))\n",
    "\n",
    "        # Check if 'url' key exists in the JSON data\n",
    "        if 'name' not in data:\n",
    "            return jsonify({'error': 'The JSON data must contain a \"name\" key'}), 400\n",
    "\n",
    "        index_name = data['name']\n",
    "\n",
    "        if index_name not in pinecone.list_indexes():\n",
    "            pinecone.create_index(\n",
    "                index_name,\n",
    "                dimension=768,\n",
    "                metric=\"cosine\"\n",
    "            )\n",
    "\n",
    "        index = pinecone.Index(index_name)\n",
    "\n",
    "        if 'passage_text' not in data:\n",
    "            return jsonify({'error': 'No passages found'}), 400\n",
    "\n",
    "        passage_texts = data['passage_text']\n",
    "        df = pd.DataFrame({'passage_text': passage_texts})\n",
    "\n",
    "        # Drop rows with less than 2 words\n",
    "        df = df[df['passage_text'].apply(lambda x: len(x.split()) >= 2)]\n",
    "\n",
    "        batch_size = len(df)\n",
    "        emb = retriever.encode(df[\"passage_text\"].tolist()).tolist()\n",
    "        meta = df.to_dict(orient=\"records\")\n",
    "        ids = [f\"{idx}\" for idx in range(len(df))]\n",
    "        to_upsert = list(zip(ids, emb, meta))\n",
    "\n",
    "        index = pinecone.Index(index_name)\n",
    "\n",
    "        for i in tqdm(range(len(to_upsert))):\n",
    "            _ = index.upsert(vectors=[to_upsert[i]])\n",
    "\n",
    "        index_stats = index.describe_index_stats()\n",
    "        return jsonify({'message': f'{index_name} vectorized'})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred: \", e)\n",
    "        return jsonify({'error': 'Internal Server Error'}), 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t=2023-10-01T06:26:43-0400 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=\"/Users/notAdmin/Library/Application Support/ngrok/ngrok.yml\" legacy_path=/Users/notAdmin/.ngrok2/ngrok.yml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * ngrok tunnel \"NgrokTunnel: \"https://ccef-37-120-244-62.ngrok-free.app\" -> \"http://localhost:5000\"\" -> \"http://127.0.0.1:5000\"\n"
     ]
    }
   ],
   "source": [
    "# launch server\n",
    "flask_thread = threading.Thread(target=run_app)\n",
    "flask_thread.start()\n",
    "\n",
    "# Open an ngrok tunnel to the HTTP server\n",
    "public_url = ngrok.connect(5000)\n",
    "print(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:5000\\\"\".format(public_url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': '125',\n",
      "              'metadata': {'passage_text': 'The tribes of the Mongol steppe '\n",
      "                                           'had no fixed succession system, '\n",
      "                                           'but often defaulted to some form '\n",
      "                                           'of ultimogeniture—succession of '\n",
      "                                           'the youngest son—on the grounds '\n",
      "                                           'that unlike his older brothers, '\n",
      "                                           'the youngest son would not have '\n",
      "                                           'had time to gain a following for '\n",
      "                                           'himself and needed the help of his '\n",
      "                                           \"father's inheritance.[163] \"\n",
      "                                           'However, it has been noticed that '\n",
      "                                           'this inheritance applied only to '\n",
      "                                           'property, not to titles.[164] '\n",
      "                                           'Through the Mongol appanage '\n",
      "                                           'system, Genghis allocated lands '\n",
      "                                           'and populations as property to '\n",
      "                                           'each member of his close family. '\n",
      "                                           'His brothers Qasar, Hachiun, '\n",
      "                                           'Temüge, and Belgutei were given '\n",
      "                                           'lands along the Greater Khingan '\n",
      "                                           'mountains in the east,[165] and '\n",
      "                                           'the lands of his three elder sons '\n",
      "                                           'were located in the west: for '\n",
      "                                           'Jochi, along the Irtysh river, '\n",
      "                                           'extending into Siberia and the '\n",
      "                                           'territory of the Kipchaks; for '\n",
      "                                           'Chagatai, the former Qara Khitai '\n",
      "                                           'territories surrounding Almaligh '\n",
      "                                           'in Turkestan; for Ogedei, lands in '\n",
      "                                           'Dzungaria;[e] and for Tolui the '\n",
      "                                           'Mongolian fatherland near the '\n",
      "                                           'Altai Mountains, as per '\n",
      "                                           'tradition.[167]'},\n",
      "              'score': 0.60600996,\n",
      "              'values': []},\n",
      "             {'id': '2',\n",
      "              'metadata': {'passage_text': 'Genghis Khan'},\n",
      "              'score': 0.581512332,\n",
      "              'values': []},\n",
      "             {'id': '131',\n",
      "              'metadata': {'passage_text': 'Genghis Khan never allowed his '\n",
      "                                           'image to be depicted in any '\n",
      "                                           'medium; as a result, any painting, '\n",
      "                                           'sculpture, or engravings are '\n",
      "                                           'interpretations of the writings of '\n",
      "                                           'historians, most writing long '\n",
      "                                           'after his death.[180] The two '\n",
      "                                           'earliest statements come from the '\n",
      "                                           'Persian chronicler Juzjani, who '\n",
      "                                           'relied on Khorasani eyewitnesses, '\n",
      "                                           'and the contemporary Song diplomat '\n",
      "                                           'Zhao Hong—both record that he was '\n",
      "                                           'tall and strong with a powerful '\n",
      "                                           'stature. Juzjani additionally '\n",
      "                                           'remarks on the khan\\'s \"cat\\'s '\n",
      "                                           'eyes\", mirroring a similar '\n",
      "                                           \"statement Dei Sechen, Genghis' \"\n",
      "                                           'father-in-law, is recorded saying '\n",
      "                                           'on meeting him as a nine-year '\n",
      "                                           'old.[181] Another written '\n",
      "                                           'description is found in the Jami '\n",
      "                                           'al-tawarikh, which states that '\n",
      "                                           'Genghis Khan and his Borjigin '\n",
      "                                           'ancestors had blue-green eyes and '\n",
      "                                           'either red hair or a ruddy '\n",
      "                                           'complexion which Kublai Khan did '\n",
      "                                           'not inherit. The factual nature of '\n",
      "                                           'this description is considered '\n",
      "                                           'controversial.[182][183]'},\n",
      "              'score': 0.580113828,\n",
      "              'values': []}],\n",
      " 'namespace': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [01/Oct/2023 06:27:32] \"POST /ask HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# # Shutdown Api\n",
    "# if server:\n",
    "#     server.shutdown()\n",
    "# # Disconnect the ngrok tunnel\n",
    "# ngrok.disconnect(public_url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rand",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
